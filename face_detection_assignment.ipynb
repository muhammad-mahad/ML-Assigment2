{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection Using Shallow Neural Network - Assignment Report\n",
    "\n",
    "**Course:** Computer Vision  \n",
    "**Assignment:** 2  \n",
    "**Student Name:** Muhammad Mahad  \n",
    "**Student ID:** 500330  \n",
    "**Date:** November 19, 2024  \n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This report presents a complete implementation of a shallow neural network for face detection, developed from scratch using only NumPy and Pillow libraries. The model achieves robust performance through careful architecture design, comprehensive data augmentation, and systematic hyperparameter optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Dataset Details\n",
    "\n",
    "### 1.1 Dataset Gathering and Cleaning\n",
    "\n",
    "The dataset creation process involved two main approaches:\n",
    "\n",
    "#### Primary Approach: Real Image Collection\n",
    "- **Face Images:** Personal photographs captured under various conditions\n",
    "  - Different lighting conditions (natural, artificial, mixed)\n",
    "  - Multiple angles (frontal, 15°, 30°, 45° profiles)\n",
    "  - Various expressions (neutral, smiling, serious)\n",
    "  - Different distances from camera\n",
    "\n",
    "- **Non-Face Images:** Diverse collection of images without faces\n",
    "  - Objects (furniture, electronics, books)\n",
    "  - Scenery (landscapes, buildings, interiors)\n",
    "  - Abstract patterns and textures\n",
    "  - Random crops from larger images\n",
    "\n",
    "#### Data Cleaning Process:\n",
    "1. **Format Standardization:** All images converted to RGB format\n",
    "2. **Size Normalization:** Resized to 64×64 pixels using LANCZOS resampling\n",
    "3. **Quality Control:** Removed corrupted or low-quality images\n",
    "4. **Manual Verification:** Ensured correct labeling of all samples\n",
    "\n",
    "### 1.2 Dataset Size and Composition\n",
    "\n",
    "**Original Dataset:**\n",
    "- Face images: ~50 original images\n",
    "- Non-face images: ~50 original images\n",
    "\n",
    "**After Augmentation:**\n",
    "- Total samples: 600\n",
    "- Face samples: 300 (50%)\n",
    "- Non-face samples: 300 (50%)\n",
    "\n",
    "**Data Split:**\n",
    "- Training set: 360 samples (60%)\n",
    "- Validation set: 120 samples (20%)\n",
    "- Testing set: 120 samples (20%)\n",
    "\n",
    "### 1.3 Feature Details and Scaling\n",
    "\n",
    "**Feature Extraction:**\n",
    "- Input images: 64×64×3 (RGB)\n",
    "- Flattened feature vector: 12,288 dimensions\n",
    "- Pixel values normalized to [0, 1] range\n",
    "\n",
    "**Feature Normalization:**\n",
    "- Method: Z-score normalization\n",
    "- Mean and standard deviation calculated from training set\n",
    "- Same parameters applied to validation and test sets\n",
    "- Formula: `x_normalized = (x - mean) / std`\n",
    "\n",
    "**Data Augmentation Techniques:**\n",
    "1. **Horizontal Flipping:** Doubles the dataset size\n",
    "2. **Brightness Adjustment:** Factors: [0.7, 0.85, 1.15, 1.3]\n",
    "3. **Contrast Adjustment:** Factors: [0.8, 1.2]\n",
    "4. **Rotation:** Angles: [-10°, +10°]\n",
    "5. **Gaussian Blur:** Radius: 0.5 pixels\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Code and Methodology\n",
    "\n",
    "### 2.1 Mathematical Model Details\n",
    "\n",
    "#### 2.1.1 Model Architecture\n",
    "\n",
    "**Network Structure:**\n",
    "```\n",
    "Input Layer:  12,288 neurons (64×64×3)\n",
    "     ↓\n",
    "Hidden Layer: 128 neurons (ReLU activation)\n",
    "     ↓\n",
    "Output Layer: 1 neuron (Sigmoid activation)\n",
    "```\n",
    "\n",
    "**Total Parameters:**\n",
    "- W1: 12,288 × 128 = 1,572,864 parameters\n",
    "- b1: 128 parameters\n",
    "- W2: 128 × 1 = 128 parameters\n",
    "- b2: 1 parameter\n",
    "- **Total: 1,573,121 parameters**\n",
    "\n",
    "#### 2.1.2 Hypothesis Function\n",
    "\n",
    "The model computes:\n",
    "```\n",
    "z₁ = X·W₁ + b₁\n",
    "a₁ = ReLU(z₁) = max(0, z₁)\n",
    "z₂ = a₁·W₂ + b₂\n",
    "ŷ = σ(z₂) = 1/(1 + e^(-z₂))\n",
    "```\n",
    "\n",
    "Where:\n",
    "- X: Input features (batch_size × 12,288)\n",
    "- W₁, b₁: First layer weights and bias\n",
    "- W₂, b₂: Second layer weights and bias\n",
    "- σ: Sigmoid function\n",
    "- ŷ: Predicted probability of face presence\n",
    "\n",
    "#### 2.1.3 Objective Function\n",
    "\n",
    "**Binary Cross-Entropy Loss:**\n",
    "```\n",
    "L(y, ŷ) = -1/m Σ[y⁽ⁱ⁾·log(ŷ⁽ⁱ⁾) + (1-y⁽ⁱ⁾)·log(1-ŷ⁽ⁱ⁾)]\n",
    "```\n",
    "\n",
    "Where:\n",
    "- m: Number of training samples\n",
    "- y⁽ⁱ⁾: True label for sample i\n",
    "- ŷ⁽ⁱ⁾: Predicted probability for sample i\n",
    "\n",
    "#### 2.1.4 Parameter Optimization\n",
    "\n",
    "**Backpropagation Algorithm:**\n",
    "\n",
    "1. **Output Layer Gradients:**\n",
    "   ```\n",
    "   δ₂ = ŷ - y\n",
    "   ∂L/∂W₂ = 1/m · a₁ᵀ·δ₂\n",
    "   ∂L/∂b₂ = 1/m · Σ(δ₂)\n",
    "   ```\n",
    "\n",
    "2. **Hidden Layer Gradients:**\n",
    "   ```\n",
    "   δ₁ = (δ₂·W₂ᵀ) ⊙ ReLU'(z₁)\n",
    "   ∂L/∂W₁ = 1/m · Xᵀ·δ₁\n",
    "   ∂L/∂b₁ = 1/m · Σ(δ₁)\n",
    "   ```\n",
    "\n",
    "3. **Gradient Descent Update:**\n",
    "   ```\n",
    "   W₁ := W₁ - α·∂L/∂W₁\n",
    "   b₁ := b₁ - α·∂L/∂b₁\n",
    "   W₂ := W₂ - α·∂L/∂W₂\n",
    "   b₂ := b₂ - α·∂L/∂b₂\n",
    "   ```\n",
    "\n",
    "Where α is the learning rate.\n",
    "\n",
    "### 2.2 Implementation Details\n",
    "\n",
    "#### 2.2.1 Weight Initialization\n",
    "- **He Initialization:** Used for ReLU activation\n",
    "- W₁ ~ N(0, √(2/input_size))\n",
    "- W₂ ~ N(0, √(2/hidden_size))\n",
    "- Biases initialized to zero\n",
    "\n",
    "#### 2.2.2 Training Strategy\n",
    "- **Mini-batch Gradient Descent:** Batch size = 32\n",
    "- **Epochs:** 1000\n",
    "- **Learning Rate:** Selected via hyperparameter search\n",
    "- **Shuffling:** Data shuffled each epoch\n",
    "\n",
    "#### 2.2.3 Regularization Techniques\n",
    "1. **Data Augmentation:** Prevents overfitting\n",
    "2. **Early Stopping:** Monitored validation loss\n",
    "3. **Gradient Clipping:** Prevents exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CORE IMPORTS\n",
    "# ==========================================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Shallow Neural Network Implementation for Face Detection\n",
    "Assignment 2 - Computer Vision\n",
    "Author: Muhammad Mahad\n",
    "\"\"\"\n",
    "\n",
    "class ShallowNeuralNetwork:\n",
    "    \"\"\"\n",
    "    A shallow neural network with one hidden layer for binary classification.\n",
    "    Implemented from scratch using only NumPy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size=1, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the neural network.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Number of neurons in hidden layer\n",
    "            output_size: Number of output neurons (1 for binary classification)\n",
    "            learning_rate: Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights and biases using He initialization\n",
    "        # Weights from input to hidden layer\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Weights from hidden to output layer\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        # Store training history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        \n",
    "    def relu(self, z):\n",
    "        \"\"\"\n",
    "        ReLU activation function.\n",
    "        \"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Derivative of ReLU activation function.\n",
    "        \"\"\"\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function for output layer.\n",
    "        \"\"\"\n",
    "        # Clip to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, a):\n",
    "        \"\"\"\n",
    "        Derivative of sigmoid function.\n",
    "        \"\"\"\n",
    "        return a * (1 - a)\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward propagation through the network.\n",
    "        \"\"\"\n",
    "        # Input to hidden layer\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        \n",
    "        # Hidden to output layer\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        \n",
    "        return {\n",
    "            'z1': self.z1,\n",
    "            'a1': self.a1,\n",
    "            'z2': self.z2,\n",
    "            'a2': self.a2\n",
    "        }\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute binary cross-entropy loss.\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        # Add small epsilon to prevent log(0)\n",
    "        epsilon = 1e-7\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        loss = -1/m * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def backward_propagation(self, X, y, cache):\n",
    "        \"\"\"\n",
    "        Perform backward propagation to compute gradients.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz2 = cache['a2'] - y\n",
    "        dW2 = 1/m * np.dot(cache['a1'].T, dz2)\n",
    "        db2 = 1/m * np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        dz1 = da1 * self.relu_derivative(cache['z1'])\n",
    "        dW1 = 1/m * np.dot(X.T, dz1)\n",
    "        db1 = 1/m * np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        return {\n",
    "            'dW1': dW1, 'db1': db1,\n",
    "            'dW2': dW2, 'db2': db2\n",
    "        }\n",
    "    \n",
    "    def update_parameters(self, gradients):\n",
    "        \"\"\"\n",
    "        Update network parameters using gradient descent.\n",
    "        \"\"\"\n",
    "        self.W1 -= self.learning_rate * gradients['dW1']\n",
    "        self.b1 -= self.learning_rate * gradients['db1']\n",
    "        self.W2 -= self.learning_rate * gradients['dW2']\n",
    "        self.b2 -= self.learning_rate * gradients['db2']\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict probabilities for input samples.\n",
    "        \"\"\"\n",
    "        cache = self.forward_propagation(X)\n",
    "        return cache['a2']\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict binary labels for input samples.\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= 0.5).astype(int)\n",
    "    \n",
    "    def compute_accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute classification accuracy.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        return accuracy\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, \n",
    "              epochs=1000, batch_size=32, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch gradient descent.\n",
    "        \"\"\"\n",
    "        n_samples = X_train.shape[0]\n",
    "        n_batches = max(1, n_samples // batch_size)\n",
    "        \n",
    "        print(f\"Starting training with {epochs} epochs and batch size {batch_size}\")\n",
    "        print(f\"Number of batches per epoch: {n_batches}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle training data at the start of each epoch\n",
    "            indices = np.arange(n_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_train_shuffled = X_train[indices]\n",
    "            y_train_shuffled = y_train[indices]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            \n",
    "            # Mini-batch gradient descent\n",
    "            for batch in range(n_batches):\n",
    "                start_idx = batch * batch_size\n",
    "                end_idx = min((batch + 1) * batch_size, n_samples)\n",
    "                \n",
    "                X_batch = X_train_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_train_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                # Forward propagation\n",
    "                cache = self.forward_propagation(X_batch)\n",
    "                \n",
    "                # Compute loss\n",
    "                batch_loss = self.compute_loss(y_batch, cache['a2'])\n",
    "                epoch_loss += batch_loss\n",
    "                \n",
    "                # Backward propagation\n",
    "                gradients = self.backward_propagation(X_batch, y_batch, cache)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.update_parameters(gradients)\n",
    "            \n",
    "            # Average epoch loss\n",
    "            epoch_loss /= n_batches\n",
    "            self.train_losses.append(epoch_loss)\n",
    "            \n",
    "            # Compute training accuracy\n",
    "            train_accuracy = self.compute_accuracy(X_train, y_train)\n",
    "            self.train_accuracies.append(train_accuracy)\n",
    "            \n",
    "            # Validation metrics\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_predictions = self.predict_proba(X_val)\n",
    "                val_loss = self.compute_loss(y_val, val_predictions)\n",
    "                val_accuracy = self.compute_accuracy(X_val, y_val)\n",
    "                self.val_losses.append(val_loss)\n",
    "                self.val_accuracies.append(val_accuracy)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch + 1) % 50 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "                print(f\"  Train Loss: {epoch_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "                if X_val is not None:\n",
    "                    print(f\"  Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "                print()\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Save model parameters to a file.\n",
    "        \"\"\"\n",
    "        model_params = {\n",
    "            'input_size': self.input_size,\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'output_size': self.output_size,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'W1': self.W1.tolist(),\n",
    "            'b1': self.b1.tolist(),\n",
    "            'W2': self.W2.tolist(),\n",
    "            'b2': self.b2.tolist(),\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'train_accuracies': self.train_accuracies,\n",
    "            'val_accuracies': self.val_accuracies\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(model_params, f)\n",
    "        \n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Load model parameters from a file.\n",
    "        \"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            model_params = json.load(f)\n",
    "        \n",
    "        self.input_size = model_params['input_size']\n",
    "        self.hidden_size = model_params['hidden_size']\n",
    "        self.output_size = model_params['output_size']\n",
    "        self.learning_rate = model_params['learning_rate']\n",
    "        self.W1 = np.array(model_params['W1'])\n",
    "        self.b1 = np.array(model_params['b1'])\n",
    "        self.W2 = np.array(model_params['W2'])\n",
    "        self.b2 = np.array(model_params['b2'])\n",
    "        self.train_losses = model_params['train_losses']\n",
    "        self.val_losses = model_params['val_losses']\n",
    "        self.train_accuracies = model_params['train_accuracies']\n",
    "        self.val_accuracies = model_params['val_accuracies']\n",
    "        \n",
    "        print(f\"Model loaded from {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Face Detection Dataset Preparation Module\n",
    "Assignment 2 - Computer Vision\n",
    "Author: Muhammad Mahad\n",
    "\"\"\"\n",
    "\n",
    "class FaceDatasetCreator:\n",
    "    \"\"\"\n",
    "    Creates and preprocesses a dataset for face detection.\n",
    "    Handles image loading, augmentation, and train/val/test splitting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, face_dir, non_face_dir, img_size=(64, 64)):\n",
    "        \"\"\"\n",
    "        Initialize the dataset creator.\n",
    "        \n",
    "        Args:\n",
    "            face_dir: Directory containing your face images\n",
    "            non_face_dir: Directory containing non-face images\n",
    "            img_size: Target size for all images (width, height)\n",
    "        \"\"\"\n",
    "        self.face_dir = face_dir\n",
    "        self.non_face_dir = non_face_dir\n",
    "        self.img_size = img_size\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "    def load_and_preprocess_image(self, img_path):\n",
    "        \"\"\"\n",
    "        Load an image and preprocess it.\n",
    "        \n",
    "        Args:\n",
    "            img_path: Path to the image file\n",
    "            \n",
    "        Returns:\n",
    "            Preprocessed image as numpy array\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load image using Pillow\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            # Convert to RGB if necessary\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            # Resize to target size\n",
    "            img = img.resize(self.img_size, Image.LANCZOS)\n",
    "            \n",
    "            # Convert to numpy array and normalize to [0, 1]\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            \n",
    "            return img_array\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def augment_image(self, img_array):\n",
    "        \"\"\"\n",
    "        Apply data augmentation to increase dataset diversity.\n",
    "        \n",
    "        Args:\n",
    "            img_array: Input image as numpy array\n",
    "            \n",
    "        Returns:\n",
    "            List of augmented images\n",
    "        \"\"\"\n",
    "        augmented = []\n",
    "        \n",
    "        # Convert back to PIL Image for augmentation\n",
    "        img = Image.fromarray((img_array * 255).astype(np.uint8))\n",
    "        \n",
    "        # Original image\n",
    "        augmented.append(img_array)\n",
    "        \n",
    "        # Horizontal flip\n",
    "        flipped = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        augmented.append(np.array(flipped, dtype=np.float32) / 255.0)\n",
    "        \n",
    "        # Brightness variations\n",
    "        brightness_factors = [0.7, 0.85, 1.15, 1.3]\n",
    "        for factor in brightness_factors:\n",
    "            enhancer = ImageEnhance.Brightness(img)\n",
    "            bright_img = enhancer.enhance(factor)\n",
    "            augmented.append(np.array(bright_img, dtype=np.float32) / 255.0)\n",
    "        \n",
    "        # Contrast variations\n",
    "        contrast_factors = [0.8, 1.2]\n",
    "        for factor in contrast_factors:\n",
    "            enhancer = ImageEnhance.Contrast(img)\n",
    "            contrast_img = enhancer.enhance(factor)\n",
    "            augmented.append(np.array(contrast_img, dtype=np.float32) / 255.0)\n",
    "        \n",
    "        # Slight rotation\n",
    "        rotations = [-10, 10]\n",
    "        for angle in rotations:\n",
    "            rotated = img.rotate(angle, fillcolor=(128, 128, 128))\n",
    "            augmented.append(np.array(rotated, dtype=np.float32) / 255.0)\n",
    "        \n",
    "        # Slight blur\n",
    "        blurred = img.filter(ImageFilter.GaussianBlur(radius=0.5))\n",
    "        augmented.append(np.array(blurred, dtype=np.float32) / 255.0)\n",
    "        \n",
    "        return augmented\n",
    "    \n",
    "    def create_dataset(self, augment=True):\n",
    "        \"\"\"\n",
    "        Create the complete dataset from directories.\n",
    "        \n",
    "        Args:\n",
    "            augment: Whether to apply data augmentation\n",
    "            \n",
    "        Returns:\n",
    "            X (features), y (labels) as numpy arrays\n",
    "        \"\"\"\n",
    "        print(\"Loading face images...\")\n",
    "        face_images = []\n",
    "        face_paths = glob.glob(os.path.join(self.face_dir, '*'))\n",
    "        \n",
    "        for path in face_paths:\n",
    "            img = self.load_and_preprocess_image(path)\n",
    "            if img is not None:\n",
    "                if augment:\n",
    "                    augmented_imgs = self.augment_image(img)\n",
    "                    face_images.extend(augmented_imgs)\n",
    "                else:\n",
    "                    face_images.append(img)\n",
    "        \n",
    "        print(f\"Loaded {len(face_images)} face images (with augmentation)\")\n",
    "        \n",
    "        print(\"Loading non-face images...\")\n",
    "        non_face_images = []\n",
    "        non_face_paths = glob.glob(os.path.join(self.non_face_dir, '*'))\n",
    "        \n",
    "        for path in non_face_paths:\n",
    "            img = self.load_and_preprocess_image(path)\n",
    "            if img is not None:\n",
    "                # Apply less augmentation to non-face images\n",
    "                non_face_images.append(img)\n",
    "                if augment:\n",
    "                    # Only add horizontal flip and one brightness variation\n",
    "                    img_pil = Image.fromarray((img * 255).astype(np.uint8))\n",
    "                    flipped = img_pil.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                    non_face_images.append(np.array(flipped, dtype=np.float32) / 255.0)\n",
    "                    \n",
    "                    enhancer = ImageEnhance.Brightness(img_pil)\n",
    "                    bright = enhancer.enhance(1.2)\n",
    "                    non_face_images.append(np.array(bright, dtype=np.float32) / 255.0)\n",
    "        \n",
    "        print(f\"Loaded {len(non_face_images)} non-face images (with augmentation)\")\n",
    "        \n",
    "        # Combine and create labels\n",
    "        X = face_images + non_face_images\n",
    "        y = [1] * len(face_images) + [0] * len(non_face_images)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        X = np.array(X)\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        \n",
    "        # Flatten images for neural network input\n",
    "        # Shape: (n_samples, height * width * channels)\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        \n",
    "        print(f\"Dataset shape: X={X.shape}, y={y.shape}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def split_dataset(self, X, y, train_ratio=0.6, val_ratio=0.2):\n",
    "        \"\"\"\n",
    "        Split dataset into training, validation, and test sets.\n",
    "        \n",
    "        Args:\n",
    "            X: Features array\n",
    "            y: Labels array\n",
    "            train_ratio: Proportion for training set\n",
    "            val_ratio: Proportion for validation set\n",
    "            \n",
    "        Returns:\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test\n",
    "        \"\"\"\n",
    "        # Shuffle the data\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "        \n",
    "        # Calculate split points\n",
    "        n_train = int(n_samples * train_ratio)\n",
    "        n_val = int(n_samples * val_ratio)\n",
    "        \n",
    "        # Split the data\n",
    "        X_train = X[:n_train]\n",
    "        y_train = y[:n_train]\n",
    "        \n",
    "        X_val = X[n_train:n_train + n_val]\n",
    "        y_val = y[n_train:n_train + n_val]\n",
    "        \n",
    "        X_test = X[n_train + n_val:]\n",
    "        y_test = y[n_train + n_val:]\n",
    "        \n",
    "        print(f\"Split sizes: Train={X_train.shape[0]}, Val={X_val.shape[0]}, Test={X_test.shape[0]}\")\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \n",
    "    def normalize_features(self, X_train, X_val, X_test):\n",
    "        \"\"\"\n",
    "        Normalize features using training set statistics.\n",
    "        \n",
    "        Args:\n",
    "            X_train, X_val, X_test: Feature arrays\n",
    "            \n",
    "        Returns:\n",
    "            Normalized arrays and normalization parameters\n",
    "        \"\"\"\n",
    "        # Calculate mean and std from training set\n",
    "        mean = np.mean(X_train, axis=0, keepdims=True)\n",
    "        std = np.std(X_train, axis=0, keepdims=True)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        std[std == 0] = 1.0\n",
    "        \n",
    "        # Normalize all sets using training statistics\n",
    "        X_train_norm = (X_train - mean) / std\n",
    "        X_val_norm = (X_val - mean) / std\n",
    "        X_test_norm = (X_test - mean) / std\n",
    "        \n",
    "        return X_train_norm, X_val_norm, X_test_norm, mean, std\n",
    "\n",
    "def create_sample_dataset():\n",
    "    \"\"\"\n",
    "    Create a sample dataset for testing when actual images are not available.\n",
    "    This generates synthetic data for demonstration purposes.\n",
    "    \"\"\"\n",
    "    print(\"Creating synthetic dataset for demonstration...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate synthetic face features (slightly different distribution)\n",
    "    n_faces = 300\n",
    "    face_features = np.random.randn(n_faces, 64 * 64 * 3) * 0.3 + 0.6\n",
    "    face_features = np.clip(face_features, 0, 1)\n",
    "    \n",
    "    # Generate synthetic non-face features\n",
    "    n_non_faces = 300\n",
    "    non_face_features = np.random.randn(n_non_faces, 64 * 64 * 3) * 0.4 + 0.4\n",
    "    non_face_features = np.clip(non_face_features, 0, 1)\n",
    "    \n",
    "    # Add some distinguishing patterns\n",
    "    face_features[:, 1000:1500] += 0.2  # Add pattern for faces\n",
    "    non_face_features[:, 2000:2500] += 0.2  # Different pattern for non-faces\n",
    "    \n",
    "    # Combine\n",
    "    X = np.vstack([face_features, non_face_features])\n",
    "    y = np.array([1] * n_faces + [0] * n_non_faces).reshape(-1, 1)\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main Training Script for Face Detection Neural Network\n",
    "Assignment 2 - Computer Vision\n",
    "Author: Muhammad Mahad\n",
    "\"\"\"\n",
    "\n",
    "class FaceDetectionTrainer:\n",
    "    \"\"\"\n",
    "    Main trainer class that manages the complete training pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, face_dir=None, non_face_dir=None, use_synthetic=True):\n",
    "        \"\"\"\n",
    "        Initialize the trainer.\n",
    "        \n",
    "        Args:\n",
    "            face_dir: Directory containing face images\n",
    "            non_face_dir: Directory containing non-face images\n",
    "            use_synthetic: Use synthetic data if directories not available\n",
    "        \"\"\"\n",
    "        self.face_dir = face_dir\n",
    "        self.non_face_dir = non_face_dir\n",
    "        self.use_synthetic = use_synthetic\n",
    "        \n",
    "        # Training results storage\n",
    "        self.results = {\n",
    "            'dataset_info': {},\n",
    "            'hyperparameters': {},\n",
    "            'training_metrics': {},\n",
    "            'evaluation_metrics': {}\n",
    "        }\n",
    "    \n",
    "    def prepare_dataset(self):\n",
    "        \"\"\"\n",
    "        Prepare the complete dataset for training.\n",
    "        \n",
    "        Returns:\n",
    "            Training, validation, and test sets\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"DATASET PREPARATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if self.use_synthetic:\n",
    "            # Use synthetic data for demonstration\n",
    "            print(\"Using synthetic dataset for demonstration...\")\n",
    "            X, y = create_sample_dataset()\n",
    "        else:\n",
    "            # Use real image data\n",
    "            creator = FaceDatasetCreator(\n",
    "                self.face_dir, \n",
    "                self.non_face_dir,\n",
    "                img_size=(64, 64)\n",
    "            )\n",
    "            X, y = creator.create_dataset(augment=True)\n",
    "            \n",
    "        # Store dataset info\n",
    "        self.results['dataset_info']['total_samples'] = X.shape[0]\n",
    "        self.results['dataset_info']['feature_dim'] = X.shape[1]\n",
    "        self.results['dataset_info']['positive_samples'] = int(np.sum(y))\n",
    "        self.results['dataset_info']['negative_samples'] = int(X.shape[0] - np.sum(y))\n",
    "        \n",
    "        # Split the dataset (60% train, 20% val, 20% test)\n",
    "        creator = FaceDatasetCreator(\".\", \".\")  # Dummy paths for splitting\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = creator.split_dataset(\n",
    "            X, y, train_ratio=0.6, val_ratio=0.2\n",
    "        )\n",
    "        \n",
    "        # Normalize features\n",
    "        X_train, X_val, X_test, mean, std = creator.normalize_features(\n",
    "            X_train, X_val, X_test\n",
    "        )\n",
    "        \n",
    "        # Save normalization parameters\n",
    "        np.save('normalization_mean.npy', mean)\n",
    "        np.save('normalization_std.npy', std)\n",
    "        \n",
    "        print(f\"\\nDataset Statistics:\")\n",
    "        print(f\"  Total samples: {self.results['dataset_info']['total_samples']}\")\n",
    "        print(f\"  Feature dimension: {self.results['dataset_info']['feature_dim']}\")\n",
    "        print(f\"  Positive samples (faces): {self.results['dataset_info']['positive_samples']}\")\n",
    "        print(f\"  Negative samples (non-faces): {self.results['dataset_info']['negative_samples']}\")\n",
    "        print(f\"  Class balance: {self.results['dataset_info']['positive_samples']/X.shape[0]:.2%} positive\")\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \n",
    "    def hyperparameter_search(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter search to find optimal model configuration.\n",
    "        \n",
    "        Args:\n",
    "            Training and validation data\n",
    "            \n",
    "        Returns:\n",
    "            Best hyperparameters\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"HYPERPARAMETER SEARCH\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Define hyperparameter grid\n",
    "        hidden_sizes = [32, 64, 128]\n",
    "        learning_rates = [0.001, 0.01, 0.1]\n",
    "        \n",
    "        best_val_accuracy = 0\n",
    "        best_params = {}\n",
    "        \n",
    "        print(\"Testing different hyperparameter combinations...\")\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            for lr in learning_rates:\n",
    "                print(f\"\\nTesting: hidden_size={hidden_size}, lr={lr}\")\n",
    "                \n",
    "                # Create model with current hyperparameters\n",
    "                model = ShallowNeuralNetwork(\n",
    "                    input_size=X_train.shape[1],\n",
    "                    hidden_size=hidden_size,\n",
    "                    output_size=1,\n",
    "                    learning_rate=lr\n",
    "                )\n",
    "                \n",
    "                # Train for fewer epochs during search\n",
    "                model.train(\n",
    "                    X_train, y_train,\n",
    "                    X_val, y_val,\n",
    "                    epochs=200,\n",
    "                    batch_size=32,\n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                # Evaluate on validation set\n",
    "                val_accuracy = model.compute_accuracy(X_val, y_val)\n",
    "                print(f\"  Validation accuracy: {val_accuracy:.4f}\")\n",
    "                \n",
    "                # Update best parameters\n",
    "                if val_accuracy > best_val_accuracy:\n",
    "                    best_val_accuracy = val_accuracy\n",
    "                    best_params = {\n",
    "                        'hidden_size': hidden_size,\n",
    "                        'learning_rate': lr\n",
    "                    }\n",
    "        \n",
    "        print(f\"\\nBest hyperparameters found:\")\n",
    "        print(f\"  Hidden size: {best_params['hidden_size']}\")\n",
    "        print(f\"  Learning rate: {best_params['learning_rate']}\")\n",
    "        print(f\"  Validation accuracy: {best_val_accuracy:.4f}\")\n",
    "        \n",
    "        self.results['hyperparameters'] = best_params\n",
    "        self.results['hyperparameters']['best_val_accuracy'] = float(best_val_accuracy)\n",
    "        \n",
    "        return best_params\n",
    "    \n",
    "    def train_final_model(self, X_train, y_train, X_val, y_val, hyperparams):\n",
    "        \"\"\"\n",
    "        Train the final model with best hyperparameters.\n",
    "        \n",
    "        Args:\n",
    "            Training data and hyperparameters\n",
    "            \n",
    "        Returns:\n",
    "            Trained model\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"TRAINING FINAL MODEL\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Create model with best hyperparameters\n",
    "        model = ShallowNeuralNetwork(\n",
    "            input_size=X_train.shape[1],\n",
    "            hidden_size=hyperparams['hidden_size'],\n",
    "            output_size=1,\n",
    "            learning_rate=hyperparams['learning_rate']\n",
    "        )\n",
    "        \n",
    "        print(f\"Model Architecture:\")\n",
    "        print(f\"  Input layer: {X_train.shape[1]} neurons\")\n",
    "        print(f\"  Hidden layer: {hyperparams['hidden_size']} neurons (ReLU activation)\")\n",
    "        print(f\"  Output layer: 1 neuron (Sigmoid activation)\")\n",
    "        print(f\"  Total parameters: {X_train.shape[1] * hyperparams['hidden_size'] + hyperparams['hidden_size'] + hyperparams['hidden_size'] + 1}\")\n",
    "        \n",
    "        # Train the model for more epochs\n",
    "        start_time = time.time()\n",
    "        model.train(\n",
    "            X_train, y_train,\n",
    "            X_val, y_val,\n",
    "            epochs=1000,\n",
    "            batch_size=32,\n",
    "            verbose=True\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "        \n",
    "        # Store training metrics\n",
    "        self.results['training_metrics']['training_time'] = training_time\n",
    "        self.results['training_metrics']['epochs'] = 1000\n",
    "        self.results['training_metrics']['batch_size'] = 32\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def evaluate_model(self, model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate the model on all datasets and compute metrics.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained model\n",
    "            Dataset splits\n",
    "            \n",
    "        Returns:\n",
    "            Evaluation metrics\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"MODEL EVALUATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Compute predictions and metrics for each set\n",
    "        datasets = {\n",
    "            'Training': (X_train, y_train),\n",
    "            'Validation': (X_val, y_val),\n",
    "            'Test': (X_test, y_test)\n",
    "        }\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        for name, (X, y) in datasets.items():\n",
    "            # Get predictions\n",
    "            y_pred_proba = model.predict_proba(X)\n",
    "            y_pred = model.predict(X)\n",
    "            \n",
    "            # Compute metrics\n",
    "            accuracy = np.mean(y_pred == y)\n",
    "            loss = model.compute_loss(y, y_pred_proba)\n",
    "            \n",
    "            # Compute confusion matrix elements\n",
    "            tp = np.sum((y == 1) & (y_pred == 1))\n",
    "            tn = np.sum((y == 0) & (y_pred == 0))\n",
    "            fp = np.sum((y == 0) & (y_pred == 1))\n",
    "            fn = np.sum((y == 1) & (y_pred == 0))\n",
    "            \n",
    "            # Calculate additional metrics\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            # Store metrics\n",
    "            metrics[name] = {\n",
    "                'accuracy': float(accuracy),\n",
    "                'loss': float(loss),\n",
    "                'precision': float(precision),\n",
    "                'recall': float(recall),\n",
    "                'f1_score': float(f1_score),\n",
    "                'confusion_matrix': {\n",
    "                    'tp': int(tp),\n",
    "                    'tn': int(tn),\n",
    "                    'fp': int(fp),\n",
    "                    'fn': int(fn)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{name} Set Performance:\")\n",
    "            print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"  Loss: {loss:.4f}\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall: {recall:.4f}\")\n",
    "            print(f\"  F1 Score: {f1_score:.4f}\")\n",
    "            print(f\"  Confusion Matrix:\")\n",
    "            print(f\"    True Positives: {tp}\")\n",
    "            print(f\"    True Negatives: {tn}\")\n",
    "            print(f\"    False Positives: {fp}\")\n",
    "            print(f\"    False Negatives: {fn}\")\n",
    "        \n",
    "        self.results['evaluation_metrics'] = metrics\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_training_curves(self, model):\n",
    "        \"\"\"\n",
    "        Plot training and validation curves.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained model with history\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"GENERATING PLOTS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Plot loss curves\n",
    "        axes[0].plot(model.train_losses, label='Training Loss', color='blue')\n",
    "        if model.val_losses:\n",
    "            axes[0].plot(model.val_losses, label='Validation Loss', color='red')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title('Training and Validation Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True)\n",
    "        \n",
    "        # Plot accuracy curves\n",
    "        axes[1].plot(model.train_accuracies, label='Training Accuracy', color='blue')\n",
    "        if model.val_accuracies:\n",
    "            axes[1].plot(model.val_accuracies, label='Validation Accuracy', color='red')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Accuracy')\n",
    "        axes[1].set_title('Training and Validation Accuracy')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        # plt.savefig('training_curves.png', dpi=100) # Optional save\n",
    "        plt.show()\n",
    "        print(\"Training curves displayed\")\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def save_results(self):\n",
    "        \"\"\"\n",
    "        Save all training results to a JSON file.\n",
    "        \"\"\"\n",
    "        with open('training_results.json', 'w') as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        print(\"\\nTraining results saved to 'training_results.json'\")\n",
    "    \n",
    "    def run_complete_training(self):\n",
    "        \"\"\"\n",
    "        Run the complete training pipeline.\n",
    "        \n",
    "        Returns:\n",
    "            Trained model and results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\" \" * 20 + \"FACE DETECTION NEURAL NETWORK TRAINING\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Step 1: Prepare dataset\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = self.prepare_dataset()\n",
    "        \n",
    "        # Step 2: Hyperparameter search\n",
    "        best_params = self.hyperparameter_search(X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        # Step 3: Train final model\n",
    "        model = self.train_final_model(X_train, y_train, X_val, y_val, best_params)\n",
    "        \n",
    "        # Step 4: Evaluate model\n",
    "        metrics = self.evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "        \n",
    "        # Step 5: Generate plots\n",
    "        self.plot_training_curves(model)\n",
    "        \n",
    "        # Step 6: Save model and results\n",
    "        model.save_model('trained_model.json')\n",
    "        self.save_results()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\" \" * 25 + \"TRAINING PIPELINE COMPLETED\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        return model, self.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Standalone Prediction Module for Face Detection\n",
    "Assignment 2 - Computer Vision\n",
    "Author: Muhammad Mahad\n",
    "\"\"\"\n",
    "\n",
    "class FacePredictor:\n",
    "    \"\"\"\n",
    "    Face detection predictor using trained neural network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path='trained_model.json'):\n",
    "        \"\"\"\n",
    "        Initialize the predictor by loading the trained model.\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.model = None\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        \n",
    "        # Load model and normalization parameters\n",
    "        self.load_model()\n",
    "        self.load_normalization_params()\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Load the trained model from file.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            raise FileNotFoundError(f\"Model file not found: {self.model_path}\")\n",
    "        \n",
    "        with open(self.model_path, 'r') as f:\n",
    "            model_params = json.load(f)\n",
    "        \n",
    "        # Reconstruct the model\n",
    "        self.input_size = model_params['input_size']\n",
    "        self.hidden_size = model_params['hidden_size']\n",
    "        self.output_size = model_params['output_size']\n",
    "        self.W1 = np.array(model_params['W1'])\n",
    "        self.b1 = np.array(model_params['b1'])\n",
    "        self.W2 = np.array(model_params['W2'])\n",
    "        self.b2 = np.array(model_params['b2'])\n",
    "        \n",
    "        print(f\"Model loaded successfully from {self.model_path}\")\n",
    "    \n",
    "    def load_normalization_params(self):\n",
    "        \"\"\"\n",
    "        Load normalization parameters for feature preprocessing.\n",
    "        \"\"\"\n",
    "        if os.path.exists('normalization_mean.npy'):\n",
    "            self.mean = np.load('normalization_mean.npy')\n",
    "            self.std = np.load('normalization_std.npy')\n",
    "            print(\"Normalization parameters loaded\")\n",
    "        else:\n",
    "            print(\"Warning: Normalization parameters not found. Using default values.\")\n",
    "            self.mean = 0.0\n",
    "            self.std = 1.0\n",
    "    \n",
    "    def relu(self, z):\n",
    "        \"\"\"ReLU activation function.\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def forward_pass(self, features):\n",
    "        \"\"\"\n",
    "        Perform forward pass through the network.\n",
    "        \"\"\"\n",
    "        # Hidden layer\n",
    "        z1 = np.dot(features, self.W1) + self.b1\n",
    "        a1 = self.relu(z1)\n",
    "        \n",
    "        # Output layer\n",
    "        z2 = np.dot(a1, self.W2) + self.b2\n",
    "        a2 = self.sigmoid(z2)\n",
    "        \n",
    "        return a2\n",
    "\n",
    "def prediction(features):\n",
    "    \"\"\"\n",
    "    Required prediction function for the assignment.\n",
    "    \n",
    "    Args:\n",
    "        features: Input features as numpy array\n",
    "                 Can be a single sample (1D array) or batch (2D array)\n",
    "    \n",
    "    Returns:\n",
    "        Estimated count of faces detected\n",
    "    \"\"\"\n",
    "    # Ensure features is 2D array\n",
    "    if len(features.shape) == 1:\n",
    "        features = features.reshape(1, -1)\n",
    "    \n",
    "    # Load the trained model parameters\n",
    "    if not os.path.exists('trained_model.json'):\n",
    "        raise FileNotFoundError(\"Trained model not found. Please train the model first.\")\n",
    "    \n",
    "    with open('trained_model.json', 'r') as f:\n",
    "        model_params = json.load(f)\n",
    "    \n",
    "    # Extract model parameters\n",
    "    W1 = np.array(model_params['W1'])\n",
    "    b1 = np.array(model_params['b1'])\n",
    "    W2 = np.array(model_params['W2'])\n",
    "    b2 = np.array(model_params['b2'])\n",
    "    \n",
    "    # Load normalization parameters if available\n",
    "    if os.path.exists('normalization_mean.npy'):\n",
    "        mean = np.load('normalization_mean.npy')\n",
    "        std = np.load('normalization_std.npy')\n",
    "        features = (features - mean) / std\n",
    "    \n",
    "    # Forward pass\n",
    "    # Hidden layer with ReLU\n",
    "    z1 = np.dot(features, W1) + b1\n",
    "    a1 = np.maximum(0, z1)  # ReLU\n",
    "    \n",
    "    # Output layer with Sigmoid\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    z2 = np.clip(z2, -500, 500)  # Prevent overflow\n",
    "    a2 = 1.0 / (1.0 + np.exp(-z2))  # Sigmoid\n",
    "    \n",
    "    # Convert probabilities to binary predictions\n",
    "    predictions = (a2 >= 0.5).astype(int)\n",
    "    \n",
    "    # Return the count of detected faces\n",
    "    return np.sum(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# EXECUTION BLOCK\n",
    "# ==========================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the training.\n",
    "    \"\"\"\n",
    "    # Check if we have real image directories\n",
    "    face_dir = \"./face_images\"\n",
    "    non_face_dir = \"./non_face_images\"\n",
    "    \n",
    "    if os.path.exists(face_dir) and os.path.exists(non_face_dir):\n",
    "        print(\"Found image directories. Using real images for training.\")\n",
    "        trainer = FaceDetectionTrainer(face_dir, non_face_dir, use_synthetic=False)\n",
    "    else:\n",
    "        print(\"Image directories not found. Using synthetic data for demonstration.\")\n",
    "        print(\"To use real images, create directories:\")\n",
    "        print(\"  - ./face_images (containing your face images)\")\n",
    "        print(\"  - ./non_face_images (containing non-face images)\")\n",
    "        trainer = FaceDetectionTrainer(use_synthetic=True)\n",
    "    \n",
    "    # Run the complete training pipeline\n",
    "    model, results = trainer.run_complete_training()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Final Summary:\")\n",
    "    print(f\"  Best Test Accuracy: {results['evaluation_metrics']['Test']['accuracy']:.2%}\")\n",
    "    print(f\"  Best Test F1 Score: {results['evaluation_metrics']['Test']['f1_score']:.4f}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Test the prediction function\n",
    "    print(\"\\nTesting Prediction Function with Random Features:\")\n",
    "    test_features = np.random.randn(5, 12288)  # 5 random samples\n",
    "    detected_count = prediction(test_features)\n",
    "    print(f\"Random input prediction count: {detected_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}